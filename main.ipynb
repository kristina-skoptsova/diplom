{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoXLAcqQXJ0qUF0hAco5zF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristina-skoptsova/diplom/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64HQrbHuFP4b",
        "outputId": "208af6e2-1ee4-4d31-a4c8-b69c72c574a1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit==1.44.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.44.1)\n",
            "Requirement already satisfied: tensorflow==2.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.6.1)\n",
            "Requirement already satisfied: keras==3.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.8.0)\n",
            "Requirement already satisfied: xgboost==2.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.1.4)\n",
            "Requirement already satisfied: matplotlib==3.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Collecting streamlit-authenticator==0.4.2 (from -r requirements.txt (line 9))\n",
            "  Downloading streamlit_authenticator-0.4.2-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.32.3)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (8.2.1)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (11.2.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.44.1->-r requirements.txt (line 1)) (6.4.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->-r requirements.txt (line 2)) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1->-r requirements.txt (line 5)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1->-r requirements.txt (line 5)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1->-r requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0->-r requirements.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0->-r requirements.txt (line 6)) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0->-r requirements.txt (line 6)) (0.15.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost==2.1.4->-r requirements.txt (line 7)) (2.21.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 8)) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 8)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.0->-r requirements.txt (line 8)) (3.2.3)\n",
            "Collecting bcrypt>=3.1.7 (from streamlit-authenticator==0.4.2->-r requirements.txt (line 9))\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Collecting captcha>=0.5.0 (from streamlit-authenticator==0.4.2->-r requirements.txt (line 9))\n",
            "  Downloading captcha-0.7.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: cryptography>=42.0.5 in /usr/local/lib/python3.11/dist-packages (from streamlit-authenticator==0.4.2->-r requirements.txt (line 9)) (43.0.3)\n",
            "Collecting extra-streamlit-components>=0.1.70 (from streamlit-authenticator==0.4.2->-r requirements.txt (line 9))\n",
            "  Downloading extra_streamlit_components-0.1.80-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: PyJWT>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit-authenticator==0.4.2->-r requirements.txt (line 9)) (2.10.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from streamlit-authenticator==0.4.2->-r requirements.txt (line 9)) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->-r requirements.txt (line 10)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->-r requirements.txt (line 10)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->-r requirements.txt (line 10)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->-r requirements.txt (line 10)) (2025.4.26)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.44.1->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.44.1->-r requirements.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.44.1->-r requirements.txt (line 1)) (1.40.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0->-r requirements.txt (line 2)) (0.45.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=42.0.5->streamlit-authenticator==0.4.2->-r requirements.txt (line 9)) (1.17.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.44.1->-r requirements.txt (line 1)) (4.0.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r requirements.txt (line 2)) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r requirements.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->-r requirements.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.8.0->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.8.0->-r requirements.txt (line 6)) (2.19.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=42.0.5->streamlit-authenticator==0.4.2->-r requirements.txt (line 9)) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.44.1->-r requirements.txt (line 1)) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit==1.44.1->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.44.1->-r requirements.txt (line 1)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.44.1->-r requirements.txt (line 1)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.44.1->-r requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.44.1->-r requirements.txt (line 1)) (0.25.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.8.0->-r requirements.txt (line 6)) (0.1.2)\n",
            "Downloading streamlit_authenticator-0.4.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading captcha-0.7.1-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading extra_streamlit_components-0.1.80-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: captcha, bcrypt, extra-streamlit-components, streamlit-authenticator\n",
            "Successfully installed bcrypt-4.3.0 captcha-0.7.1 extra-streamlit-components-0.1.80 streamlit-authenticator-0.4.2\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 1s\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from keras.models import load_model\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import os\n",
        "import tempfile\n",
        "from streamlit_authenticator import Authenticate\n",
        "import requests\n",
        "\n",
        "# Загрузка секретов из GitHub\n",
        "def load_secrets():\n",
        "    secrets_url = 'https://raw.githubusercontent.com/kristina-skoptsova/diplom/refs/heads/main/secrets.toml'\n",
        "    try:\n",
        "        response = requests.get(secrets_url)\n",
        "        if response.status_code == 200:\n",
        "            # Проверка содержимого файла\n",
        "            content = response.text\n",
        "            if '[credentials]' not in content:\n",
        "                st.error('Ошибка: Файл secrets.toml не содержит раздел [credentials]')\n",
        "                return\n",
        "\n",
        "            # Создание папки .streamlit и сохранение\n",
        "            streamlit_dir = os.path.expanduser('~/.streamlit')\n",
        "            os.makedirs(streamlit_dir, exist_ok=True)\n",
        "            secrets_path = os.path.join(streamlit_dir, 'secrets.toml')\n",
        "\n",
        "            with open(secrets_path, 'w') as f:\n",
        "                f.write(content)\n",
        "            st.success('Файл secrets.toml успешно загружен')\n",
        "        else:\n",
        "            st.error(f'Ошибка загрузки: {response.status_code}')\n",
        "    except Exception as e:\n",
        "        st.error(f'Ошибка при загрузке файла: {e}')\n",
        "\n",
        "# Проверка secrets.toml\n",
        "secrets_path = os.path.expanduser('~/.streamlit/secrets.toml')\n",
        "\n",
        "if not os.path.exists(secrets_path):\n",
        "    load_secrets()\n",
        "\n",
        "# Функция аутентификации\n",
        "def authenticate():\n",
        "    if 'authenticated' not in st.session_state:\n",
        "        st.session_state.authenticated = False\n",
        "\n",
        "    if not st.session_state.authenticated:\n",
        "        with st.form('auth_form'):\n",
        "            st.subheader('Авторизация')\n",
        "            username = st.text_input('Логин')\n",
        "            password = st.text_input('Пароль', type='password')\n",
        "            submit_button = st.form_submit_button('Войти')\n",
        "\n",
        "            if submit_button:\n",
        "                if 'credentials' in st.secrets:\n",
        "                    valid_users = st.secrets['credentials']\n",
        "                    if username in valid_users and valid_users[username] == password:\n",
        "                        st.session_state.authenticated = True\n",
        "                        st.rerun()\n",
        "                    else:\n",
        "                        st.error('Неверные учетные данные')\n",
        "                else:\n",
        "                    st.error('Ошибка конфигурации системы')\n",
        "        st.stop()\n",
        "    return True\n",
        "\n",
        "if authenticate():\n",
        "  # Инициализация сессии для хранения состояния\n",
        "  if 'model' not in st.session_state:\n",
        "      st.session_state.model = None\n",
        "  if 'data' not in st.session_state:\n",
        "      st.session_state.data = None\n",
        "  if 'scaler' not in st.session_state:\n",
        "      st.session_state.scaler = RobustScaler()\n",
        "  # Кнопка выхода\n",
        "      if st.sidebar.button('Выйти'):\n",
        "          st.session_state.clear()\n",
        "          st.experimental_rerun()\n",
        "\n",
        "  # Основной интерфейс\n",
        "  st.title('Система прогнозирования набора абитуриентов на направления подготовки высшего образования')\n",
        "  # Создание вкладок\n",
        "  tab1, tab2, tab3 = st.tabs(['Загрузка данных', 'Обучение модели', 'Тестирование модели'])\n",
        "\n",
        "  # Вкладка 1: Загрузка данных\n",
        "  with tab1:\n",
        "      st.header('Загрузка данных')\n",
        "      st.write('Загрузите файл CSV с данными для анализа.')\n",
        "      uploaded_file = st.file_uploader('Выберите файл CSV', type=['csv'])\n",
        "      if uploaded_file is not None:\n",
        "          data = pd.read_csv(uploaded_file)\n",
        "          st.write('Первые 5 строк загруженного файла:')\n",
        "          st.dataframe(data.head())\n",
        "\n",
        "          # Проверка наличия необходимых столбцов\n",
        "          required_columns = {'Направление', 'Год поступления', 'Количество поступивших'}\n",
        "          if not required_columns.issubset(data.columns):\n",
        "              st.error(f'Ошибка: В данных отсутствуют необходимые столбцы: {required_columns}')\n",
        "          else:\n",
        "            # Проверка на пропущенные значения\n",
        "            if data[list(required_columns)].isnull().any().any():\n",
        "                missing_values = data[list(required_columns)].isnull().sum()\n",
        "                st.error(f'Ошибка: Данные содержат пропуски в следующих столбцах:\\n{missing_values[missing_values > 0]}')\n",
        "            # Проверка на дубликаты\n",
        "            elif data.duplicated().any():\n",
        "                duplicates = data.duplicated().sum()\n",
        "                duplicate_rows = data[data.duplicated(keep=False)].sort_values(by=list(data.columns))\n",
        "                st.error(f'Ошибка: Найдено {duplicates} полных дубликатов строк')\n",
        "            else:\n",
        "                # Удаление колонок\n",
        "                columns_to_drop = ['Уровень безработицы', 'Регион рождения', 'Доля наличия договора']\n",
        "                data = data.drop(columns=columns_to_drop)\n",
        "                # Колонки, которые не нужно масштабировать\n",
        "                exclude_columns = ['Направление', 'Год поступления', 'Количество поступивших']\n",
        "                # Колонки для масштабирования\n",
        "                columns_to_scale = [col for col in data.columns if col not in exclude_columns]\n",
        "                # Применение RobustScaler\n",
        "                scaler = RobustScaler()\n",
        "                data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
        "                # Сохранение данных\n",
        "                st.session_state.data = data\n",
        "\n",
        "  # Вкладка 2: Обучение модели\n",
        "  with tab2:\n",
        "      st.header('Обучение модели')\n",
        "      if st.session_state.data is None:\n",
        "          st.warning(\"Сначала загрузите данные на вкладке 'Загрузка данных'\")\n",
        "      else:\n",
        "          if st.button('Начать обучение'):\n",
        "              with st.spinner('Обучение модели...'):\n",
        "                  try:\n",
        "                      import tensorflow as tf\n",
        "                      from keras.models import Sequential\n",
        "                      from keras.layers import Dense, Input, LSTM, Dropout\n",
        "                      from sklearn.model_selection import train_test_split\n",
        "                      from sklearn.preprocessing import RobustScaler\n",
        "                      from keras.callbacks import EarlyStopping\n",
        "\n",
        "                      X = st.session_state.data.drop(columns=['Количество поступивших'])\n",
        "                      y = st.session_state.data['Количество поступивших']\n",
        "                      # Функция для создания последовательностей данных\n",
        "                      def create_sequences(data, window_size):\n",
        "                          inputs, outputs, groups = [], [], []\n",
        "                          for direction in data['Направление'].unique():\n",
        "                              dir_data = data[data['Направление'] == direction].sort_values('Год поступления')\n",
        "                              for i in range(len(dir_data) - window_size):\n",
        "                                  seq = dir_data.iloc[i:i+window_size].drop(['Направление', 'Год поступления', 'Количество поступивших'], axis=1).values\n",
        "                                  target = dir_data.iloc[i+window_size]['Количество поступивших']\n",
        "                                  direction_label = dir_data.iloc[i+window_size]['Направление']  # Направление для следующего шага\n",
        "                                  inputs.append(seq)\n",
        "                                  outputs.append(target)\n",
        "                                  groups.append(direction_label)\n",
        "                          return np.array(inputs), np.array(outputs), np.array(groups)\n",
        "\n",
        "                      # Создание последовательностей\n",
        "                      YEARS_SIZE = 5\n",
        "                      X, y, direction_labels = create_sequences(st.session_state.data, YEARS_SIZE)\n",
        "\n",
        "                      # Нормализация целевой переменной\n",
        "                      y_scaler = RobustScaler()\n",
        "                      y = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "                      # Разделение данных на тренировочную и тестовую выборки\n",
        "                      X_train, X_test, y_train, y_test, direction_train, direction_test = train_test_split(\n",
        "                          X, y, direction_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "                      # Сохрание x_test и y_test\n",
        "                      st.session_state.X_test = X_test\n",
        "                      st.session_state.y_test = y_test\n",
        "\n",
        "                      # Рассчитываются веса только для тренировочной выборки\n",
        "                      unique_directions, direction_counts_train = np.unique(direction_train, return_counts=True)\n",
        "                      direction_weights_train = {direction: 1.0 / count for direction, count in zip(unique_directions, direction_counts_train)}\n",
        "\n",
        "                      # Применение весов к данным\n",
        "                      sample_weights = st.session_state.data['Направление'].map(direction_weights_train).values\n",
        "\n",
        "                      # Ограничение веса для тренировочной выборки\n",
        "                      train_sample_weights = sample_weights[:len(y_train)]\n",
        "\n",
        "                      # Архитектура модели\n",
        "                      model_lstm = Sequential([\n",
        "                          Input(shape=(YEARS_SIZE, X.shape[2])),\n",
        "                          LSTM(128, return_sequences=True),\n",
        "                          Dropout(0.1),\n",
        "                          LSTM(64),\n",
        "                          Dense(32, activation='relu'),\n",
        "                          Dense(1)\n",
        "                      ])\n",
        "\n",
        "                      # Компиляция модели\n",
        "                      model_lstm.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "\n",
        "                      # Ранняя остановка при переобучении\n",
        "                      early_stopping = EarlyStopping(\n",
        "                          monitor='val_loss',\n",
        "                          patience=10,\n",
        "                          restore_best_weights=True  # Восстановление весов модели с лучшим результатом\n",
        "                      )\n",
        "\n",
        "                      # Обучение модели\n",
        "                      history_lstm = model_lstm.fit(\n",
        "                          X_train,\n",
        "                          y_train,\n",
        "                          sample_weight=train_sample_weights,  # Использование веса\n",
        "                          epochs=100,\n",
        "                          batch_size=64,\n",
        "                          validation_split=0.2,\n",
        "                          callbacks=[early_stopping],\n",
        "                          verbose=1\n",
        "                      )\n",
        "\n",
        "                      # Сохранение модели\n",
        "                      model_lstm.save('lstm_model.keras')\n",
        "                      st.success('Модель успешно обучена и сохранена!')\n",
        "                      st.session_state.model = model_lstm\n",
        "\n",
        "                      # Сохранение scaler для предсказаний\n",
        "                      st.session_state.scaler = y_scaler\n",
        "\n",
        "                      # Визуализация обучения\n",
        "                      st.line_chart(pd.DataFrame(history_lstm.history))\n",
        "\n",
        "                      # Возможность скачивания файла\n",
        "                      with open('lstm_model.keras', 'rb') as file:\n",
        "                        st.download_button(\n",
        "                            label='Скачать модель',\n",
        "                            data=file,\n",
        "                            file_name='lstm_model.keras',\n",
        "                            mime='application/octet-stream'\n",
        "                            )\n",
        "                  except Exception as e:\n",
        "                    st.error(f'Ошибка при обучении модели: {e}')\n",
        "\n",
        "  # Вкладка 3: Тестирование модели\n",
        "  with tab3:\n",
        "      st.header('Тестирование модели')\n",
        "      # Загрузка модели\n",
        "      st.subheader('Загрузка модели')\n",
        "      uploaded_model = st.file_uploader('Выберите файл модели (.keras)', type=['keras'])\n",
        "      if uploaded_model:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.keras') as tmp_file:\n",
        "          tmp_file.write(uploaded_model.getvalue())\n",
        "          st.session_state.model = load_model(tmp_file.name)\n",
        "        st.success('Модель успешно загружена!')\n",
        "\n",
        "      if st.session_state.model is None:\n",
        "          st.warning('Сначала загрузите или обучите модель')\n",
        "      else:\n",
        "          # Подраздел: Оценка модели\n",
        "          st.subheader('Оценка модели')\n",
        "          if st.button('Оценить модель'):\n",
        "              try:\n",
        "                  # Извлечение тестовых данных\n",
        "                  X_test = st.session_state.X_test\n",
        "                  y_test = st.session_state.y_test\n",
        "\n",
        "                  # Предсказания на тестовых данных\n",
        "                  predictions = st.session_state.model.predict(X_test)\n",
        "                  predictions = st.session_state.scaler.inverse_transform(predictions)\n",
        "                  y_test_original = st.session_state.scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "                  # Оценка качества модели\n",
        "                  from sklearn.metrics import mean_absolute_error, r2_score\n",
        "                  mae = mean_absolute_error(y_test_original, predictions)\n",
        "                  r2 = r2_score(y_test_original, predictions)\n",
        "                  st.write(f'Средняя абсолютная ошибка (MAE): {mae:.2f}')\n",
        "                  st.write(f'Коэффициент детерминации (R²): {r2:.2f}')\n",
        "\n",
        "                  # Отображение результатов\n",
        "                  results = pd.DataFrame({\n",
        "                      'Реальные значения': y_test_original.flatten(),\n",
        "                      'Предсказанные значения': predictions.flatten()\n",
        "                  })\n",
        "                  st.write('Результаты предсказания на тестовых данных:')\n",
        "                  st.dataframe(results)\n",
        "\n",
        "              except Exception as e:\n",
        "                  st.error(f'Ошибка при оценке модели: {e}')\n",
        "          # Подраздел: Прогнозирование\n",
        "          st.subheader('Прогнозирование')\n",
        "          all_directions = st.session_state.data['Направление'].unique().tolist()\n",
        "          forecast_option = st.selectbox('Выберите направления для прогнозирования:', ['Все'] + all_directions)\n",
        "          selected_directions = all_directions if forecast_option == \"Все\" else [forecast_option]\n",
        "          if st.button('Запустить прогнозирование'):\n",
        "              try:\n",
        "                  # Функция для предсказания следующего года\n",
        "                  def predict_next_year(model, df, years_size, y_scaler):\n",
        "                      predictions = []\n",
        "\n",
        "                      for direction in df['Направление'].unique():\n",
        "                          if selected_directions != 'Все' and direction not in selected_directions:\n",
        "                              continue\n",
        "\n",
        "                          dir_data = df[df['Направление'] == direction].sort_values('Год поступления')\n",
        "\n",
        "                          # Если данных недостаточно для создания последовательности, пропускаем направление\n",
        "                          if len(dir_data) < years_size:\n",
        "                              st.write(f'Недостаточно данных для направления {direction}. Требуется минимум {years_size} года.')\n",
        "                              continue\n",
        "\n",
        "                          # Создаем последовательность из последних years_size лет\n",
        "                          last_sequence = dir_data.iloc[-years_size:].drop(['Направление', 'Год поступления', 'Количество поступивших'], axis=1).values\n",
        "                          last_sequence = last_sequence.reshape(1, years_size, last_sequence.shape[1])\n",
        "\n",
        "                          # Нормализуем последовательность\n",
        "                          scaler = RobustScaler()\n",
        "                          last_sequence = scaler.fit_transform(last_sequence.reshape(-1, last_sequence.shape[2])).reshape(last_sequence.shape)\n",
        "\n",
        "                          # Предсказываем количество абитуриентов\n",
        "                          predicted_value = model.predict(last_sequence)\n",
        "\n",
        "                          # Преобразуем предсказание обратно в исходный масштаб\n",
        "                          predicted_value = y_scaler.inverse_transform(predicted_value)[0][0]\n",
        "\n",
        "                          # Сохраняем результат\n",
        "                          predictions.append({\n",
        "                              'Направление': direction,\n",
        "                              'Предсказанное количество абитуриентов': int(round(predicted_value))\n",
        "                          })\n",
        "\n",
        "                      # Создаем DataFrame с результатами\n",
        "                      result_df = pd.DataFrame(predictions)\n",
        "                      return result_df\n",
        "\n",
        "                  # Вызываем функцию предсказания\n",
        "                  YEARS_SIZE = 5\n",
        "                  predictions_df = predict_next_year(st.session_state.model, st.session_state.data, YEARS_SIZE, st.session_state.scaler)\n",
        "\n",
        "                  if predictions_df.empty:\n",
        "                      st.error('Нет данных для выбранных направлений или недостаточно записей для прогнозирования.')\n",
        "                  else:\n",
        "                      # Отображение результатов\n",
        "                      st.write('Прогноз количества абитуриентов на следующий год:')\n",
        "                      st.dataframe(predictions_df)\n",
        "\n",
        "                      # Сохранение результатов прогноза\n",
        "                      predictions_df.to_csv('results.csv', index=False)\n",
        "                      st.success('Результаты прогноза успешно сохранены в файл results.csv!')\n",
        "                      # Возможность скачивания файла\n",
        "                      with open('results.csv', 'rb') as file:\n",
        "                        st.download_button(\n",
        "                            label='Скачать результаты прогноза',\n",
        "                            data=file,\n",
        "                            file_name='results.csv',\n",
        "                            mime='text/csv'\n",
        "                            )\n",
        "              except Exception as e:\n",
        "                  st.error(f'Ошибка при прогнозировании: {e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atykb5eAFU38",
        "outputId": "aa4804aa-b4ba-42cd-d116-79f12fc10ec9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Пароль:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aymPBocoFc_6",
        "outputId": "7b5eca3d-174a-4de9-86ce-8166e34dd3a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пароль: 34.16.157.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 &>/dev/null & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Supdo2gYFdyw",
        "outputId": "577ac613-1c9d-4f8f-84a4-f35cc718b27d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://big-hornets-drop.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}